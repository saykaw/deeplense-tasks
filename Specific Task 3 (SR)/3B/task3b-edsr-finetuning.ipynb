{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11118138,"sourceType":"datasetVersion","datasetId":6932733},{"sourceId":11118840,"sourceType":"datasetVersion","datasetId":6933273},{"sourceId":11155496,"sourceType":"datasetVersion","datasetId":6960160},{"sourceId":301579,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":257559,"modelId":278854},{"sourceId":301581,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":257561,"modelId":278856},{"sourceId":301582,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":257562,"modelId":278857},{"sourceId":301583,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":257563,"modelId":278858},{"sourceId":301584,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":257564,"modelId":278859}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch-optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T15:01:05.247569Z","iopub.execute_input":"2025-03-25T15:01:05.247903Z","iopub.status.idle":"2025-03-25T15:01:08.557114Z","shell.execute_reply.started":"2025-03-25T15:01:05.247844Z","shell.execute_reply":"2025-03-25T15:01:08.555998Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch-optimizer in /usr/local/lib/python3.10/dist-packages (0.3.0)\nRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from torch-optimizer) (2.5.1+cu121)\nRequirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from torch-optimizer) (0.1.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch-optimizer) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch-optimizer) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch-optimizer) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch-optimizer) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch-optimizer) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch-optimizer) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch-optimizer) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->torch-optimizer) (3.0.2)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision.transforms import ToTensor\nimport torch.cuda.amp as amp\nfrom torchvision import transforms, models\n\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom torch_optimizer import RAdam \nimport torchmetrics\nfrom torchmetrics.image import StructuralSimilarityIndexMeasure\nfrom torchmetrics.image import PeakSignalNoiseRatio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:29:36.290720Z","iopub.execute_input":"2025-03-25T14:29:36.291070Z","iopub.status.idle":"2025-03-25T14:29:36.296458Z","shell.execute_reply.started":"2025-03-25T14:29:36.291040Z","shell.execute_reply":"2025-03-25T14:29:36.295444Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"torch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:27:39.632757Z","iopub.execute_input":"2025-03-25T14:27:39.633262Z","iopub.status.idle":"2025-03-25T14:27:39.640291Z","shell.execute_reply.started":"2025-03-25T14:27:39.633224Z","shell.execute_reply":"2025-03-25T14:27:39.639507Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class RealLensingDataset(Dataset):\n    def __init__(self, lr_paths, hr_paths, augment=False, denoise=False):\n        \"\"\"\n        Args:\n            lr_paths: List of paths to LR .npy files.\n            hr_paths: List of paths to HR .npy files.\n            augment: If True, apply random flips and rotations.\n            denoise: If True, apply Gaussian blur to reduce noise.\n        \"\"\"\n        self.lr_paths = lr_paths\n        self.hr_paths = hr_paths\n        self.augment = augment\n        self.denoise = denoise\n        self.denoise_transform = transforms.GaussianBlur(kernel_size=3, sigma=1.0) if denoise else None\n        self.augment_transforms = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n            transforms.RandomRotation(degrees=(0, 360), interpolation=transforms.InterpolationMode.BILINEAR)\n        ]) if augment else None\n    \n    def __len__(self):\n        return len(self.lr_paths)\n    \n    def __getitem__(self, idx):\n        lr = np.load(self.lr_paths[idx]).astype(np.float32)\n        hr = np.load(self.hr_paths[idx]).astype(np.float32)\n        \n        # Normalize to [0, 1]\n        lr_min, lr_max = lr.min(), lr.max()\n        if lr_max > lr_min:\n            lr = (lr - lr_min) / (lr_max - lr_min)\n        else:\n            lr = np.zeros_like(lr)\n        hr_min, hr_max = hr.min(), hr.max()\n        if hr_max > hr_min:\n            hr = (hr - hr_min) / (hr_max - hr_min)\n        else:\n            hr = np.zeros_like(hr)\n        \n        lr = torch.from_numpy(lr)\n        hr = torch.from_numpy(hr)\n        \n        # Apply denoising if enabled\n        if self.denoise and self.denoise_transform:\n            lr = self.denoise_transform(lr)\n            hr = self.denoise_transform(hr)\n        \n        # Apply augmentation if enabled\n        if self.augment and self.augment_transforms:\n            seed = torch.randint(0, 2**32, (1,)).item()\n            torch.manual_seed(seed)\n            lr = self.augment_transforms(lr)\n            torch.manual_seed(seed)\n            hr = self.augment_transforms(hr)\n        \n        return lr, hr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:27:40.713463Z","iopub.execute_input":"2025-03-25T14:27:40.713752Z","iopub.status.idle":"2025-03-25T14:27:40.723518Z","shell.execute_reply.started":"2025-03-25T14:27:40.713731Z","shell.execute_reply":"2025-03-25T14:27:40.722581Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"lr_dir = \"/kaggle/input/dataset-image-super-resolution-3b/Dataset/LR\"\nhr_dir = \"/kaggle/input/dataset-image-super-resolution-3b/Dataset/HR\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:27:43.181244Z","iopub.execute_input":"2025-03-25T14:27:43.181578Z","iopub.status.idle":"2025-03-25T14:27:43.185045Z","shell.execute_reply.started":"2025-03-25T14:27:43.181545Z","shell.execute_reply":"2025-03-25T14:27:43.184286Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"lr_files = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith('.npy')])\nhr_files = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith('.npy')])\nassert len(lr_files) == len(hr_files) == 300, \"LR/HR mismatch\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:27:44.325389Z","iopub.execute_input":"2025-03-25T14:27:44.325689Z","iopub.status.idle":"2025-03-25T14:27:44.334693Z","shell.execute_reply.started":"2025-03-25T14:27:44.325666Z","shell.execute_reply":"2025-03-25T14:27:44.333897Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"indices = list(range(300))\ntrain_idx, temp_idx = train_test_split(indices, test_size=60, random_state=42)\nval_idx, test_idx = train_test_split(temp_idx, test_size=30, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:27:45.979572Z","iopub.execute_input":"2025-03-25T14:27:45.979911Z","iopub.status.idle":"2025-03-25T14:27:45.985226Z","shell.execute_reply.started":"2025-03-25T14:27:45.979852Z","shell.execute_reply":"2025-03-25T14:27:45.984393Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Datasets for different stages\ndataset_noaug = RealLensingDataset(lr_files, hr_files, augment=False, denoise=False)\ndataset_aug = RealLensingDataset(lr_files, hr_files, augment=True, denoise=False)\ndataset_aug_denoise = RealLensingDataset(lr_files, hr_files, augment=True, denoise=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:27:47.441516Z","iopub.execute_input":"2025-03-25T14:27:47.441799Z","iopub.status.idle":"2025-03-25T14:27:47.446312Z","shell.execute_reply.started":"2025-03-25T14:27:47.441777Z","shell.execute_reply":"2025-03-25T14:27:47.445573Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_dataset_noaug = Subset(dataset_noaug, train_idx)\nval_dataset = Subset(dataset_noaug, val_idx)  # No aug for val/test\ntest_dataset = Subset(dataset_noaug, test_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:27:48.522116Z","iopub.execute_input":"2025-03-25T14:27:48.522411Z","iopub.status.idle":"2025-03-25T14:27:48.526325Z","shell.execute_reply.started":"2025-03-25T14:27:48.522390Z","shell.execute_reply":"2025-03-25T14:27:48.525447Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_dataset_aug = Subset(dataset_aug, train_idx)\ntrain_dataset_aug_denoise = Subset(dataset_aug_denoise, train_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:27:49.537714Z","iopub.execute_input":"2025-03-25T14:27:49.538057Z","iopub.status.idle":"2025-03-25T14:27:49.541893Z","shell.execute_reply.started":"2025-03-25T14:27:49.538031Z","shell.execute_reply":"2025-03-25T14:27:49.540828Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_loader_noaug = DataLoader(train_dataset_noaug, batch_size=8, shuffle=True, num_workers=8, pin_memory=True, prefetch_factor=2)\ntrain_loader_aug = DataLoader(train_dataset_aug, batch_size=8, shuffle=True, num_workers=8, pin_memory=True, prefetch_factor=2)\ntrain_loader_aug_denoise = DataLoader(train_dataset_aug_denoise, batch_size=8, shuffle=True, num_workers=8, pin_memory=True, prefetch_factor=2)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8, pin_memory=True, prefetch_factor=2)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=8, pin_memory=True, prefetch_factor=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:28:00.620142Z","iopub.execute_input":"2025-03-25T14:28:00.620442Z","iopub.status.idle":"2025-03-25T14:28:00.625424Z","shell.execute_reply.started":"2025-03-25T14:28:00.620419Z","shell.execute_reply":"2025-03-25T14:28:00.624598Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class PerceptualLoss(nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features[:29].eval()\n        self.vgg = vgg.to(\"cuda\")\n        for param in self.vgg.parameters():\n            param.requires_grad = False\n        self.mse = nn.MSELoss()\n    \n    def forward(self, sr, hr):\n        sr = sr.repeat(1, 3, 1, 1)\n        hr = hr.repeat(1, 3, 1, 1)\n        sr_features = self.vgg(sr)\n        hr_features = self.vgg(hr)\n        return self.mse(sr_features, hr_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:28:09.320500Z","iopub.execute_input":"2025-03-25T14:28:09.320923Z","iopub.status.idle":"2025-03-25T14:28:09.328773Z","shell.execute_reply.started":"2025-03-25T14:28:09.320887Z","shell.execute_reply":"2025-03-25T14:28:09.327903Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, channels=64):\n        super(ResBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n    \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        return out + 0.3 * residual  \n        \nclass EDSR(nn.Module):\n    def __init__(self, scale_factor=2, num_blocks=8, channels=64):  # 8 blocks\n        super(EDSR, self).__init__()\n        self.input_conv = nn.Conv2d(1, channels, 3, padding=1, bias=False)\n        self.res_blocks = nn.Sequential(*[ResBlock(channels) for _ in range(num_blocks)])\n        self.mid_conv = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n        self.upsample = nn.Sequential(\n            nn.Conv2d(channels, channels*4, 3, padding=1, bias=False),\n            nn.PixelShuffle(2)\n        )\n        self.output_conv = nn.Conv2d(channels, 1, 3, padding=1, bias=False)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n    \n    def forward(self, x):\n        x = self.input_conv(x)\n        residual = x\n        x = self.res_blocks(x)\n        x = self.mid_conv(x) + 0.3 * residual\n        x = self.upsample(x)\n        x = self.output_conv(x)\n        return torch.clamp(x, 0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:28:12.568758Z","iopub.execute_input":"2025-03-25T14:28:12.569065Z","iopub.status.idle":"2025-03-25T14:28:12.576989Z","shell.execute_reply.started":"2025-03-25T14:28:12.569041Z","shell.execute_reply":"2025-03-25T14:28:12.576056Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#Args:\n#model: EDSR model to fine-tune\n#train_loader: DataLoader for training data\n#val_loader: DataLoader for validation dat\n#epochs: Number of epochs to train\n#lr: Learning rate\n#use_perceptual: If True, use perceptual loss\n#use_radam: If True, use RAdam optimizer\n#use_scheduler: If True, use CosineAnnealingWarmRestarts\n#stage_name: Name of the stage for logging\n\ndef finetune(model, train_loader, val_loader, epochs, lr=5e-4, use_perceptual=False, use_radam=False, use_scheduler=False, stage_name=\"Stage\"):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    # Loss functions\n    l1_criterion = nn.L1Loss()\n    mse_criterion = nn.MSELoss()\n    perceptual_criterion = PerceptualLoss() if use_perceptual else None\n    \n    # Optimizer\n    if use_radam:\n        optimizer = RAdam(model.parameters(), lr=lr, weight_decay=1e-4)\n    else:\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    \n    # Scheduler\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=1) if use_scheduler else None\n    \n    scaler = torch.amp.GradScaler('cuda')\n    train_losses, val_losses = [], []\n    best_val_loss = float('inf')\n    \n    # Debug output\n    print(f\"\\n=== {stage_name} ===\")\n    print(f\"Model on device: {next(model.parameters()).device}\")\n    print(f\"Initial VRAM usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n    lr, hr = next(iter(train_loader))\n    print(f\"LR shape: {lr.shape}, HR shape: {hr.shape}\")\n    with torch.no_grad():\n        sr = model(lr.to(device))\n        print(f\"SR shape: {sr.shape}, SR min: {sr.min().item():.4f}, max: {sr.max().item():.4f}\")\n    \n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for lr, hr in train_loader:\n            lr, hr = lr.to(device), hr.to(device)\n            optimizer.zero_grad()\n            with torch.amp.autocast('cuda'):\n                sr = model(lr)\n                l1_loss = l1_criterion(sr, hr)\n                loss = l1_loss\n                if use_perceptual:\n                    mse_loss = mse_criterion(sr, hr)\n                    perc_loss = perceptual_criterion(sr, hr)\n                    loss = l1_loss + 0.1 * mse_loss + (0.7 if stage_name == \"Stage 5\" else 0.5) * perc_loss\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            train_loss += l1_loss.item() * lr.size(0)\n        train_loss /= len(train_loader.dataset)\n        train_losses.append(train_loss)\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for lr, hr in val_loader:\n                lr, hr = lr.to(device), hr.to(device)\n                with torch.amp.autocast('cuda'):\n                    sr = model(lr)\n                    loss = l1_criterion(sr, hr)\n                val_loss += loss.item() * lr.size(0)\n        val_loss /= len(val_loader.dataset)\n        val_losses.append(val_loss)\n        \n        # Scheduler step\n        if scheduler:\n            scheduler.step()\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n        \n        if (epoch + 1) % 5 == 0:\n            with torch.no_grad():\n                sr = model(lr.to(device))\n                print(f\"Epoch {epoch+1} SR min: {sr.min().item():.4f}, max: {sr.max().item():.4f}\")\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), f\"edsr_finetuned_best_{stage_name}.pth\")\n    \n    plt.figure(figsize=(10, 5))\n    plt.plot(range(1, epochs+1), train_losses, label=\"Train Loss\")\n    plt.plot(range(1, epochs+1), val_losses, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"L1 Loss\")\n    plt.title(f\"{stage_name} Fine-Tuning Loss Curves\")\n    plt.legend()\n    plt.grid()\n    plt.savefig(f\"{stage_name}_loss_curves.png\")\n    plt.close()\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:28:18.745225Z","iopub.execute_input":"2025-03-25T14:28:18.745540Z","iopub.status.idle":"2025-03-25T14:28:18.757739Z","shell.execute_reply.started":"2025-03-25T14:28:18.745515Z","shell.execute_reply":"2025-03-25T14:28:18.756812Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def evaluate_split(model, loader, split_name, num_vis=3):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n    \n    mse_metric = torchmetrics.MeanSquaredError().to(device)\n    ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n    psnr_metric = torchmetrics.PeakSignalNoiseRatio(data_range=1.0).to(device)\n    \n    vis_lr, vis_hr, vis_sr = [], [], []\n    with torch.no_grad():\n        for i, (lr, hr) in enumerate(loader):\n            lr, hr = lr.to(device), hr.to(device)\n            sr = model(lr)\n            mse_metric.update(sr, hr)\n            ssim_metric.update(sr, hr)\n            psnr_metric.update(sr, hr)\n            if i < num_vis:\n                vis_lr.append(lr.cpu())\n                vis_hr.append(hr.cpu())\n                vis_sr.append(sr.cpu())\n    \n    mse = mse_metric.compute()\n    ssim = ssim_metric.compute()\n    psnr = psnr_metric.compute()\n    print(f\"{split_name} MSE: {mse:.4f}, SSIM: {ssim:.4f}, PSNR: {psnr:.4f}\")\n    \n    for i in range(num_vis):\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # 1 row, 3 columns\n        axes[0].imshow(vis_lr[i][0, 0], cmap=\"gray\")\n        axes[0].set_title(f\"{split_name} LR\")\n        axes[1].imshow(vis_sr[i][0, 0], cmap=\"gray\")\n        axes[1].set_title(f\"{split_name} SR\")\n        axes[2].imshow(vis_hr[i][0, 0], cmap=\"gray\")\n        axes[2].set_title(f\"{split_name} HR\")\n        for ax in axes:\n            ax.axis(\"off\")\n        plt.tight_layout()\n        plt.savefig(f\"sr_visuals_{split_name.lower()}_sidebyside_{i+1}.png\")\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:28:22.341615Z","iopub.execute_input":"2025-03-25T14:28:22.341953Z","iopub.status.idle":"2025-03-25T14:28:22.349580Z","shell.execute_reply.started":"2025-03-25T14:28:22.341924Z","shell.execute_reply":"2025-03-25T14:28:22.348732Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Task 1 model\nmodel = EDSR(scale_factor=2, num_blocks=8, channels=64)\nmodel.load_state_dict(torch.load(\"/kaggle/input/edsr-best-pth/edsr_best.pth\", weights_only=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:28:27.592600Z","iopub.execute_input":"2025-03-25T14:28:27.592914Z","iopub.status.idle":"2025-03-25T14:28:27.830680Z","shell.execute_reply.started":"2025-03-25T14:28:27.592891Z","shell.execute_reply":"2025-03-25T14:28:27.829950Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# Stage 1: Basic Fine-Tuning (No Augmentation, L1 Loss)\n# - Start with the Task 1 model, fine-tune on real data with minimal changes.\n# - No augmentation to establish a baseline.\n# - Expected: PSNR ~28-29 dB, SSIM ~0.8 due to real data noise.\nmodel = finetune(model, train_loader_noaug, val_loader, epochs=50, lr=5e-4, use_perceptual=False, use_radam=False, use_scheduler=False, stage_name=\"Stage1\")\nevaluate_split(model, train_loader_noaug, \"Train_Stage1\", num_vis=3)\nevaluate_split(model, val_loader, \"Val_Stage1\", num_vis=3)\nevaluate_split(model, test_loader, \"Test_Stage1\", num_vis=3)\ntorch.save(model.state_dict(), \"edsr_finetuned_stage1.pth\")\nprint(\"Final fine-tuned model saved as edsr_finetuned_stage1.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:30:41.312622Z","iopub.execute_input":"2025-03-25T14:30:41.312931Z","iopub.status.idle":"2025-03-25T14:31:37.951330Z","shell.execute_reply.started":"2025-03-25T14:30:41.312906Z","shell.execute_reply":"2025-03-25T14:31:37.950482Z"}},"outputs":[{"name":"stdout","text":"\n=== Stage1 ===\nModel on device: cuda:0\nInitial VRAM usage: 13.75 MB\nLR shape: torch.Size([8, 1, 64, 64]), HR shape: torch.Size([8, 1, 128, 128])\nSR shape: torch.Size([8, 1, 128, 128]), SR min: 0.0032, max: 0.2334\nEpoch 1/50, Train Loss: 0.0172, Val Loss: 0.0166\nEpoch 2/50, Train Loss: 0.0172, Val Loss: 0.0162\nEpoch 3/50, Train Loss: 0.0174, Val Loss: 0.0164\nEpoch 4/50, Train Loss: 0.0175, Val Loss: 0.0162\nEpoch 5/50, Train Loss: 0.0176, Val Loss: 0.0180\nEpoch 5 SR min: 0.0009, max: 0.1845\nEpoch 6/50, Train Loss: 0.0172, Val Loss: 0.0161\nEpoch 7/50, Train Loss: 0.0176, Val Loss: 0.0161\nEpoch 8/50, Train Loss: 0.0172, Val Loss: 0.0167\nEpoch 9/50, Train Loss: 0.0173, Val Loss: 0.0168\nEpoch 10/50, Train Loss: 0.0173, Val Loss: 0.0163\nEpoch 10 SR min: 0.0011, max: 0.2311\nEpoch 11/50, Train Loss: 0.0172, Val Loss: 0.0161\nEpoch 12/50, Train Loss: 0.0176, Val Loss: 0.0169\nEpoch 13/50, Train Loss: 0.0174, Val Loss: 0.0171\nEpoch 14/50, Train Loss: 0.0174, Val Loss: 0.0170\nEpoch 15/50, Train Loss: 0.0172, Val Loss: 0.0166\nEpoch 15 SR min: 0.0011, max: 0.2201\nEpoch 16/50, Train Loss: 0.0172, Val Loss: 0.0165\nEpoch 17/50, Train Loss: 0.0175, Val Loss: 0.0164\nEpoch 18/50, Train Loss: 0.0172, Val Loss: 0.0161\nEpoch 19/50, Train Loss: 0.0175, Val Loss: 0.0161\nEpoch 20/50, Train Loss: 0.0172, Val Loss: 0.0166\nEpoch 20 SR min: 0.0010, max: 0.2192\nEpoch 21/50, Train Loss: 0.0173, Val Loss: 0.0164\nEpoch 22/50, Train Loss: 0.0173, Val Loss: 0.0166\nEpoch 23/50, Train Loss: 0.0173, Val Loss: 0.0165\nEpoch 24/50, Train Loss: 0.0172, Val Loss: 0.0166\nEpoch 25/50, Train Loss: 0.0172, Val Loss: 0.0164\nEpoch 25 SR min: 0.0011, max: 0.2297\nEpoch 26/50, Train Loss: 0.0173, Val Loss: 0.0162\nEpoch 27/50, Train Loss: 0.0172, Val Loss: 0.0166\nEpoch 28/50, Train Loss: 0.0172, Val Loss: 0.0167\nEpoch 29/50, Train Loss: 0.0172, Val Loss: 0.0161\nEpoch 30/50, Train Loss: 0.0174, Val Loss: 0.0163\nEpoch 30 SR min: 0.0011, max: 0.2364\nEpoch 31/50, Train Loss: 0.0172, Val Loss: 0.0162\nEpoch 32/50, Train Loss: 0.0173, Val Loss: 0.0173\nEpoch 33/50, Train Loss: 0.0173, Val Loss: 0.0162\nEpoch 34/50, Train Loss: 0.0172, Val Loss: 0.0166\nEpoch 35/50, Train Loss: 0.0173, Val Loss: 0.0163\nEpoch 35 SR min: 0.0010, max: 0.2334\nEpoch 36/50, Train Loss: 0.0172, Val Loss: 0.0164\nEpoch 37/50, Train Loss: 0.0172, Val Loss: 0.0164\nEpoch 38/50, Train Loss: 0.0172, Val Loss: 0.0162\nEpoch 39/50, Train Loss: 0.0174, Val Loss: 0.0161\nEpoch 40/50, Train Loss: 0.0173, Val Loss: 0.0173\nEpoch 40 SR min: 0.0010, max: 0.1967\nEpoch 41/50, Train Loss: 0.0174, Val Loss: 0.0168\nEpoch 42/50, Train Loss: 0.0173, Val Loss: 0.0166\nEpoch 43/50, Train Loss: 0.0172, Val Loss: 0.0164\nEpoch 44/50, Train Loss: 0.0174, Val Loss: 0.0171\nEpoch 45/50, Train Loss: 0.0172, Val Loss: 0.0187\nEpoch 45 SR min: 0.0008, max: 0.1763\nEpoch 46/50, Train Loss: 0.0176, Val Loss: 0.0169\nEpoch 47/50, Train Loss: 0.0172, Val Loss: 0.0166\nEpoch 48/50, Train Loss: 0.0172, Val Loss: 0.0162\nEpoch 49/50, Train Loss: 0.0173, Val Loss: 0.0162\nEpoch 50/50, Train Loss: 0.0172, Val Loss: 0.0168\nEpoch 50 SR min: 0.0010, max: 0.2136\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n  _future_warning(\n/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n  _future_warning(\n","output_type":"stream"},{"name":"stdout","text":"Train_Stage1 MSE: 0.0021, SSIM: 0.8419, PSNR: 26.8258\nVal_Stage1 MSE: 0.0016, SSIM: 0.8380, PSNR: 28.0450\nTest_Stage1 MSE: 0.0018, SSIM: 0.8062, PSNR: 27.4101\nFinal fine-tuned model saved as edsr_finetuned_stage1.pth\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Stage 2: Add Data Augmentation\n# - Add augmentation (flips, rotations) to improve generalization to real data's variability.\n# - Reset to Task 1 model to isolate the effect of augmentation.\n# - Expected: PSNR ~29 dB, SSIM ~0.81 - slight improvement due to better generalization.\nmodel = EDSR(scale_factor=2, num_blocks=8, channels=64)\nmodel.load_state_dict(torch.load(\"/kaggle/input/edsr-best-pth/edsr_best.pth\", weights_only=True))\nmodel = finetune(model, train_loader_aug, val_loader, epochs=50, lr=5e-4, use_perceptual=False, use_radam=False, use_scheduler=False, stage_name=\"Stage2\")\nevaluate_split(model, train_loader_aug, \"Train_Stage2\", num_vis=3)\nevaluate_split(model, val_loader, \"Val_Stage2\", num_vis=3)\nevaluate_split(model, test_loader, \"Test_Stage2\", num_vis=3)\n# Save final model\ntorch.save(model.state_dict(), \"edsr_finetuned_stage2.pth\")\nprint(\"Final fine-tuned model saved as edsr_finetuned_stage2.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:35:51.184337Z","iopub.execute_input":"2025-03-25T14:35:51.184699Z","iopub.status.idle":"2025-03-25T14:36:58.425253Z","shell.execute_reply.started":"2025-03-25T14:35:51.184671Z","shell.execute_reply":"2025-03-25T14:36:58.424283Z"}},"outputs":[{"name":"stdout","text":"\n=== Stage2 ===\nModel on device: cuda:0\nInitial VRAM usage: 27.80 MB\nLR shape: torch.Size([8, 1, 64, 64]), HR shape: torch.Size([8, 1, 128, 128])\nSR shape: torch.Size([8, 1, 128, 128]), SR min: 0.0000, max: 1.0000\nEpoch 1/50, Train Loss: 0.0312, Val Loss: 0.0356\nEpoch 2/50, Train Loss: 0.0298, Val Loss: 0.0356\nEpoch 3/50, Train Loss: 0.0299, Val Loss: 0.0356\nEpoch 4/50, Train Loss: 0.0299, Val Loss: 0.0356\nEpoch 5/50, Train Loss: 0.0299, Val Loss: 0.0356\nEpoch 5 SR min: 0.0000, max: 0.0122\nEpoch 6/50, Train Loss: 0.0299, Val Loss: 0.0356\nEpoch 7/50, Train Loss: 0.0297, Val Loss: 0.0186\nEpoch 8/50, Train Loss: 0.0164, Val Loss: 0.0166\nEpoch 9/50, Train Loss: 0.0161, Val Loss: 0.0174\nEpoch 10/50, Train Loss: 0.0159, Val Loss: 0.0182\nEpoch 10 SR min: 0.0007, max: 0.1985\nEpoch 11/50, Train Loss: 0.0157, Val Loss: 0.0169\nEpoch 12/50, Train Loss: 0.0158, Val Loss: 0.0172\nEpoch 13/50, Train Loss: 0.0158, Val Loss: 0.0194\nEpoch 14/50, Train Loss: 0.0158, Val Loss: 0.0170\nEpoch 15/50, Train Loss: 0.0156, Val Loss: 0.0163\nEpoch 15 SR min: 0.0010, max: 0.2627\nEpoch 16/50, Train Loss: 0.0157, Val Loss: 0.0167\nEpoch 17/50, Train Loss: 0.0156, Val Loss: 0.0168\nEpoch 18/50, Train Loss: 0.0159, Val Loss: 0.0168\nEpoch 19/50, Train Loss: 0.0155, Val Loss: 0.0165\nEpoch 20/50, Train Loss: 0.0156, Val Loss: 0.0163\nEpoch 20 SR min: 0.0011, max: 0.2913\nEpoch 21/50, Train Loss: 0.0160, Val Loss: 0.0169\nEpoch 22/50, Train Loss: 0.0156, Val Loss: 0.0163\nEpoch 23/50, Train Loss: 0.0160, Val Loss: 0.0163\nEpoch 24/50, Train Loss: 0.0155, Val Loss: 0.0174\nEpoch 25/50, Train Loss: 0.0158, Val Loss: 0.0164\nEpoch 25 SR min: 0.0009, max: 0.2496\nEpoch 26/50, Train Loss: 0.0156, Val Loss: 0.0162\nEpoch 27/50, Train Loss: 0.0157, Val Loss: 0.0162\nEpoch 28/50, Train Loss: 0.0156, Val Loss: 0.0172\nEpoch 29/50, Train Loss: 0.0155, Val Loss: 0.0162\nEpoch 30/50, Train Loss: 0.0163, Val Loss: 0.0164\nEpoch 30 SR min: 0.0011, max: 0.2994\nEpoch 31/50, Train Loss: 0.0159, Val Loss: 0.0168\nEpoch 32/50, Train Loss: 0.0155, Val Loss: 0.0171\nEpoch 33/50, Train Loss: 0.0155, Val Loss: 0.0165\nEpoch 34/50, Train Loss: 0.0156, Val Loss: 0.0162\nEpoch 35/50, Train Loss: 0.0157, Val Loss: 0.0164\nEpoch 35 SR min: 0.0010, max: 0.2515\nEpoch 36/50, Train Loss: 0.0156, Val Loss: 0.0163\nEpoch 37/50, Train Loss: 0.0156, Val Loss: 0.0164\nEpoch 38/50, Train Loss: 0.0156, Val Loss: 0.0163\nEpoch 39/50, Train Loss: 0.0156, Val Loss: 0.0162\nEpoch 40/50, Train Loss: 0.0160, Val Loss: 0.0164\nEpoch 40 SR min: 0.0010, max: 0.2524\nEpoch 41/50, Train Loss: 0.0157, Val Loss: 0.0167\nEpoch 42/50, Train Loss: 0.0157, Val Loss: 0.0171\nEpoch 43/50, Train Loss: 0.0158, Val Loss: 0.0166\nEpoch 44/50, Train Loss: 0.0156, Val Loss: 0.0165\nEpoch 45/50, Train Loss: 0.0157, Val Loss: 0.0166\nEpoch 45 SR min: 0.0009, max: 0.2401\nEpoch 46/50, Train Loss: 0.0157, Val Loss: 0.0163\nEpoch 47/50, Train Loss: 0.0155, Val Loss: 0.0171\nEpoch 48/50, Train Loss: 0.0157, Val Loss: 0.0165\nEpoch 49/50, Train Loss: 0.0156, Val Loss: 0.0163\nEpoch 50/50, Train Loss: 0.0156, Val Loss: 0.0165\nEpoch 50 SR min: 0.0010, max: 0.2512\nTrain_Stage2 MSE: 0.0018, SSIM: 0.8621, PSNR: 27.5156\nVal_Stage2 MSE: 0.0014, SSIM: 0.8362, PSNR: 28.5168\nTest_Stage2 MSE: 0.0017, SSIM: 0.8056, PSNR: 27.7850\nFinal fine-tuned model saved as edsr_finetuned_stage2.pth\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Stage 3: Add Perceptual Loss (With Augmentation)\n# - Continue using augmentation (train_loader_aug) to maintain generalization.\n# - Add perceptual loss (VGG19, weight 0.5) to improve SSIM and push PSNR.\n# - Reset to Task 1 model to isolate the effect of perceptual loss.\n# - Expected: PSNR ~29.5 dB, SSIM ~0.82 - perceptual loss boosts structural fidelity.\nmodel = EDSR(scale_factor=2, num_blocks=8, channels=64)\nmodel.load_state_dict(torch.load(\"/kaggle/input/edsr-best-pth/edsr_best.pth\", weights_only=True))\nmodel = finetune(model, train_loader_aug, val_loader, epochs=50, lr=5e-4, use_perceptual=True, use_radam=False, use_scheduler=False, stage_name=\"Stage3\")\nevaluate_split(model, train_loader_aug, \"Train_Stage3\", num_vis=3)\nevaluate_split(model, val_loader, \"Val_Stage3\", num_vis=3)\nevaluate_split(model, test_loader, \"Test_Stage3\", num_vis=3)\n# Save final model\ntorch.save(model.state_dict(), \"edsr_finetuned_stage3.pth\")\nprint(\"Final fine-tuned model saved as edsr_finetuned_stage3.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:40:48.505135Z","iopub.execute_input":"2025-03-25T14:40:48.505474Z","iopub.status.idle":"2025-03-25T14:42:41.610323Z","shell.execute_reply.started":"2025-03-25T14:40:48.505444Z","shell.execute_reply":"2025-03-25T14:42:41.609241Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:02<00:00, 201MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"\n=== Stage3 ===\nModel on device: cuda:0\nInitial VRAM usage: 77.21 MB\nLR shape: torch.Size([8, 1, 64, 64]), HR shape: torch.Size([8, 1, 128, 128])\nSR shape: torch.Size([8, 1, 128, 128]), SR min: 0.0000, max: 1.0000\nEpoch 1/50, Train Loss: 0.0232, Val Loss: 0.0184\nEpoch 2/50, Train Loss: 0.0163, Val Loss: 0.0176\nEpoch 3/50, Train Loss: 0.0162, Val Loss: 0.0160\nEpoch 4/50, Train Loss: 0.0151, Val Loss: 0.0159\nEpoch 5/50, Train Loss: 0.0164, Val Loss: 0.0156\nEpoch 5 SR min: 0.0000, max: 1.0000\nEpoch 6/50, Train Loss: 0.0153, Val Loss: 0.0157\nEpoch 7/50, Train Loss: 0.0150, Val Loss: 0.0156\nEpoch 8/50, Train Loss: 0.0157, Val Loss: 0.0157\nEpoch 9/50, Train Loss: 0.0157, Val Loss: 0.0158\nEpoch 10/50, Train Loss: 0.0155, Val Loss: 0.0161\nEpoch 10 SR min: 0.0000, max: 0.6954\nEpoch 11/50, Train Loss: 0.0153, Val Loss: 0.0155\nEpoch 12/50, Train Loss: 0.0153, Val Loss: 0.0158\nEpoch 13/50, Train Loss: 0.0155, Val Loss: 0.0159\nEpoch 14/50, Train Loss: 0.0154, Val Loss: 0.0159\nEpoch 15/50, Train Loss: 0.0157, Val Loss: 0.0174\nEpoch 15 SR min: 0.0000, max: 1.0000\nEpoch 16/50, Train Loss: 0.0153, Val Loss: 0.0158\nEpoch 17/50, Train Loss: 0.0146, Val Loss: 0.0154\nEpoch 18/50, Train Loss: 0.0148, Val Loss: 0.0153\nEpoch 19/50, Train Loss: 0.0148, Val Loss: 0.0153\nEpoch 20/50, Train Loss: 0.0146, Val Loss: 0.0153\nEpoch 20 SR min: 0.0000, max: 1.0000\nEpoch 21/50, Train Loss: 0.0150, Val Loss: 0.0159\nEpoch 22/50, Train Loss: 0.0150, Val Loss: 0.0160\nEpoch 23/50, Train Loss: 0.0146, Val Loss: 0.0156\nEpoch 24/50, Train Loss: 0.0145, Val Loss: 0.0154\nEpoch 25/50, Train Loss: 0.0145, Val Loss: 0.0156\nEpoch 25 SR min: 0.0000, max: 1.0000\nEpoch 26/50, Train Loss: 0.0147, Val Loss: 0.0158\nEpoch 27/50, Train Loss: 0.0149, Val Loss: 0.0159\nEpoch 28/50, Train Loss: 0.0143, Val Loss: 0.0156\nEpoch 29/50, Train Loss: 0.0148, Val Loss: 0.0152\nEpoch 30/50, Train Loss: 0.0146, Val Loss: 0.0157\nEpoch 30 SR min: 0.0000, max: 1.0000\nEpoch 31/50, Train Loss: 0.0144, Val Loss: 0.0153\nEpoch 32/50, Train Loss: 0.0150, Val Loss: 0.0156\nEpoch 33/50, Train Loss: 0.0144, Val Loss: 0.0153\nEpoch 34/50, Train Loss: 0.0145, Val Loss: 0.0154\nEpoch 35/50, Train Loss: 0.0148, Val Loss: 0.0154\nEpoch 35 SR min: 0.0000, max: 1.0000\nEpoch 36/50, Train Loss: 0.0146, Val Loss: 0.0151\nEpoch 37/50, Train Loss: 0.0146, Val Loss: 0.0157\nEpoch 38/50, Train Loss: 0.0151, Val Loss: 0.0155\nEpoch 39/50, Train Loss: 0.0144, Val Loss: 0.0151\nEpoch 40/50, Train Loss: 0.0150, Val Loss: 0.0149\nEpoch 40 SR min: 0.0000, max: 0.9686\nEpoch 41/50, Train Loss: 0.0145, Val Loss: 0.0153\nEpoch 42/50, Train Loss: 0.0144, Val Loss: 0.0181\nEpoch 43/50, Train Loss: 0.0148, Val Loss: 0.0153\nEpoch 44/50, Train Loss: 0.0147, Val Loss: 0.0161\nEpoch 45/50, Train Loss: 0.0144, Val Loss: 0.0161\nEpoch 45 SR min: 0.0000, max: 1.0000\nEpoch 46/50, Train Loss: 0.0151, Val Loss: 0.0162\nEpoch 47/50, Train Loss: 0.0151, Val Loss: 0.0152\nEpoch 48/50, Train Loss: 0.0147, Val Loss: 0.0154\nEpoch 49/50, Train Loss: 0.0143, Val Loss: 0.0158\nEpoch 50/50, Train Loss: 0.0150, Val Loss: 0.0151\nEpoch 50 SR min: 0.0000, max: 0.9546\nTrain_Stage3 MSE: 0.0012, SSIM: 0.8629, PSNR: 29.2846\nVal_Stage3 MSE: 0.0007, SSIM: 0.8306, PSNR: 31.3032\nTest_Stage3 MSE: 0.0011, SSIM: 0.8038, PSNR: 29.6750\nFinal fine-tuned model saved as edsr_finetuned_stage3.pth\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Stage 4: Optimize LR and Scheduler (With Augmentation)\n# - Continue using augmentation (train_loader_aug) to maintain generalization.\n# - Increase LR to 1e-3, add CosineAnnealingWarmRestarts to avoid stagnation.\n# - Reset to Task 1 model to isolate the effect of LR/scheduler.\n# - Expected: PSNR ~30 dB, SSIM ~0.83 - better training dynamics.\nmodel = EDSR(scale_factor=2, num_blocks=8, channels=64)\nmodel.load_state_dict(torch.load(\"/kaggle/input/edsr-best-pth/edsr_best.pth\", weights_only=True))\nmodel = finetune(model, train_loader_aug, val_loader, epochs=100, lr=1e-3, use_perceptual=True, use_radam=False, use_scheduler=True, stage_name=\"Stage4\")\nevaluate_split(model, train_loader_aug, \"Train_Stage4\", num_vis=3)\nevaluate_split(model, val_loader, \"Val_Stage4\", num_vis=3)\nevaluate_split(model, test_loader, \"Test_Stage4\", num_vis=3)\ntorch.save(model.state_dict(), \"edsr_finetuned_stage4.pth\")\nprint(\"Final fine-tuned model saved as edsr_finetuned_stage4.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T14:45:02.074695Z","iopub.execute_input":"2025-03-25T14:45:02.075040Z","iopub.status.idle":"2025-03-25T14:48:37.754036Z","shell.execute_reply.started":"2025-03-25T14:45:02.075013Z","shell.execute_reply":"2025-03-25T14:48:37.753148Z"}},"outputs":[{"name":"stdout","text":"\n=== Stage4 ===\nModel on device: cuda:0\nInitial VRAM usage: 77.70 MB\nLR shape: torch.Size([8, 1, 64, 64]), HR shape: torch.Size([8, 1, 128, 128])\nSR shape: torch.Size([8, 1, 128, 128]), SR min: 0.0000, max: 1.0000\nEpoch 1/100, Train Loss: 0.0233, Val Loss: 0.0224\nEpoch 2/100, Train Loss: 0.0177, Val Loss: 0.0181\nEpoch 3/100, Train Loss: 0.0183, Val Loss: 0.0200\nEpoch 4/100, Train Loss: 0.0175, Val Loss: 0.0175\nEpoch 5/100, Train Loss: 0.0167, Val Loss: 0.0173\nEpoch 5 SR min: 0.0000, max: 1.0000\nEpoch 6/100, Train Loss: 0.0158, Val Loss: 0.0157\nEpoch 7/100, Train Loss: 0.0157, Val Loss: 0.0160\nEpoch 8/100, Train Loss: 0.0150, Val Loss: 0.0156\nEpoch 9/100, Train Loss: 0.0149, Val Loss: 0.0158\nEpoch 10/100, Train Loss: 0.0153, Val Loss: 0.0154\nEpoch 10 SR min: 0.0000, max: 1.0000\nEpoch 11/100, Train Loss: 0.0147, Val Loss: 0.0152\nEpoch 12/100, Train Loss: 0.0148, Val Loss: 0.0154\nEpoch 13/100, Train Loss: 0.0144, Val Loss: 0.0154\nEpoch 14/100, Train Loss: 0.0150, Val Loss: 0.0151\nEpoch 15/100, Train Loss: 0.0145, Val Loss: 0.0151\nEpoch 15 SR min: 0.0000, max: 1.0000\nEpoch 16/100, Train Loss: 0.0145, Val Loss: 0.0158\nEpoch 17/100, Train Loss: 0.0144, Val Loss: 0.0154\nEpoch 18/100, Train Loss: 0.0142, Val Loss: 0.0152\nEpoch 19/100, Train Loss: 0.0142, Val Loss: 0.0151\nEpoch 20/100, Train Loss: 0.0142, Val Loss: 0.0151\nEpoch 20 SR min: 0.0000, max: 1.0000\nEpoch 21/100, Train Loss: 0.0151, Val Loss: 0.0173\nEpoch 22/100, Train Loss: 0.0156, Val Loss: 0.0156\nEpoch 23/100, Train Loss: 0.0153, Val Loss: 0.0159\nEpoch 24/100, Train Loss: 0.0153, Val Loss: 0.0165\nEpoch 25/100, Train Loss: 0.0152, Val Loss: 0.0157\nEpoch 25 SR min: 0.0000, max: 1.0000\nEpoch 26/100, Train Loss: 0.0149, Val Loss: 0.0158\nEpoch 27/100, Train Loss: 0.0151, Val Loss: 0.0160\nEpoch 28/100, Train Loss: 0.0148, Val Loss: 0.0154\nEpoch 29/100, Train Loss: 0.0147, Val Loss: 0.0151\nEpoch 30/100, Train Loss: 0.0145, Val Loss: 0.0160\nEpoch 30 SR min: 0.0000, max: 1.0000\nEpoch 31/100, Train Loss: 0.0147, Val Loss: 0.0152\nEpoch 32/100, Train Loss: 0.0148, Val Loss: 0.0153\nEpoch 33/100, Train Loss: 0.0145, Val Loss: 0.0152\nEpoch 34/100, Train Loss: 0.0144, Val Loss: 0.0151\nEpoch 35/100, Train Loss: 0.0145, Val Loss: 0.0153\nEpoch 35 SR min: 0.0000, max: 1.0000\nEpoch 36/100, Train Loss: 0.0142, Val Loss: 0.0151\nEpoch 37/100, Train Loss: 0.0141, Val Loss: 0.0151\nEpoch 38/100, Train Loss: 0.0141, Val Loss: 0.0152\nEpoch 39/100, Train Loss: 0.0143, Val Loss: 0.0152\nEpoch 40/100, Train Loss: 0.0141, Val Loss: 0.0152\nEpoch 40 SR min: 0.0000, max: 1.0000\nEpoch 41/100, Train Loss: 0.0153, Val Loss: 0.0156\nEpoch 42/100, Train Loss: 0.0149, Val Loss: 0.0156\nEpoch 43/100, Train Loss: 0.0152, Val Loss: 0.0156\nEpoch 44/100, Train Loss: 0.0147, Val Loss: 0.0152\nEpoch 45/100, Train Loss: 0.0152, Val Loss: 0.0156\nEpoch 45 SR min: 0.0000, max: 1.0000\nEpoch 46/100, Train Loss: 0.0149, Val Loss: 0.0168\nEpoch 47/100, Train Loss: 0.0153, Val Loss: 0.0154\nEpoch 48/100, Train Loss: 0.0148, Val Loss: 0.0154\nEpoch 49/100, Train Loss: 0.0146, Val Loss: 0.0155\nEpoch 50/100, Train Loss: 0.0145, Val Loss: 0.0154\nEpoch 50 SR min: 0.0000, max: 1.0000\nEpoch 51/100, Train Loss: 0.0144, Val Loss: 0.0157\nEpoch 52/100, Train Loss: 0.0147, Val Loss: 0.0154\nEpoch 53/100, Train Loss: 0.0144, Val Loss: 0.0153\nEpoch 54/100, Train Loss: 0.0142, Val Loss: 0.0154\nEpoch 55/100, Train Loss: 0.0142, Val Loss: 0.0151\nEpoch 55 SR min: 0.0000, max: 1.0000\nEpoch 56/100, Train Loss: 0.0143, Val Loss: 0.0152\nEpoch 57/100, Train Loss: 0.0139, Val Loss: 0.0152\nEpoch 58/100, Train Loss: 0.0140, Val Loss: 0.0151\nEpoch 59/100, Train Loss: 0.0139, Val Loss: 0.0151\nEpoch 60/100, Train Loss: 0.0140, Val Loss: 0.0151\nEpoch 60 SR min: 0.0000, max: 1.0000\nEpoch 61/100, Train Loss: 0.0150, Val Loss: 0.0165\nEpoch 62/100, Train Loss: 0.0151, Val Loss: 0.0159\nEpoch 63/100, Train Loss: 0.0154, Val Loss: 0.0152\nEpoch 64/100, Train Loss: 0.0146, Val Loss: 0.0152\nEpoch 65/100, Train Loss: 0.0145, Val Loss: 0.0160\nEpoch 65 SR min: 0.0000, max: 1.0000\nEpoch 66/100, Train Loss: 0.0149, Val Loss: 0.0155\nEpoch 67/100, Train Loss: 0.0144, Val Loss: 0.0154\nEpoch 68/100, Train Loss: 0.0145, Val Loss: 0.0154\nEpoch 69/100, Train Loss: 0.0149, Val Loss: 0.0162\nEpoch 70/100, Train Loss: 0.0145, Val Loss: 0.0155\nEpoch 70 SR min: 0.0000, max: 0.9520\nEpoch 71/100, Train Loss: 0.0142, Val Loss: 0.0152\nEpoch 72/100, Train Loss: 0.0144, Val Loss: 0.0168\nEpoch 73/100, Train Loss: 0.0144, Val Loss: 0.0152\nEpoch 74/100, Train Loss: 0.0143, Val Loss: 0.0149\nEpoch 75/100, Train Loss: 0.0145, Val Loss: 0.0152\nEpoch 75 SR min: 0.0000, max: 0.9796\nEpoch 76/100, Train Loss: 0.0141, Val Loss: 0.0150\nEpoch 77/100, Train Loss: 0.0139, Val Loss: 0.0150\nEpoch 78/100, Train Loss: 0.0141, Val Loss: 0.0152\nEpoch 79/100, Train Loss: 0.0141, Val Loss: 0.0151\nEpoch 80/100, Train Loss: 0.0140, Val Loss: 0.0151\nEpoch 80 SR min: 0.0000, max: 1.0000\nEpoch 81/100, Train Loss: 0.0149, Val Loss: 0.0155\nEpoch 82/100, Train Loss: 0.0145, Val Loss: 0.0155\nEpoch 83/100, Train Loss: 0.0150, Val Loss: 0.0158\nEpoch 84/100, Train Loss: 0.0147, Val Loss: 0.0156\nEpoch 85/100, Train Loss: 0.0149, Val Loss: 0.0153\nEpoch 85 SR min: 0.0000, max: 1.0000\nEpoch 86/100, Train Loss: 0.0147, Val Loss: 0.0165\nEpoch 87/100, Train Loss: 0.0148, Val Loss: 0.0153\nEpoch 88/100, Train Loss: 0.0144, Val Loss: 0.0159\nEpoch 89/100, Train Loss: 0.0144, Val Loss: 0.0153\nEpoch 90/100, Train Loss: 0.0145, Val Loss: 0.0150\nEpoch 90 SR min: 0.0000, max: 1.0000\nEpoch 91/100, Train Loss: 0.0143, Val Loss: 0.0150\nEpoch 92/100, Train Loss: 0.0145, Val Loss: 0.0155\nEpoch 93/100, Train Loss: 0.0139, Val Loss: 0.0152\nEpoch 94/100, Train Loss: 0.0143, Val Loss: 0.0149\nEpoch 95/100, Train Loss: 0.0144, Val Loss: 0.0148\nEpoch 95 SR min: 0.0000, max: 1.0000\nEpoch 96/100, Train Loss: 0.0142, Val Loss: 0.0153\nEpoch 97/100, Train Loss: 0.0140, Val Loss: 0.0153\nEpoch 98/100, Train Loss: 0.0139, Val Loss: 0.0150\nEpoch 99/100, Train Loss: 0.0142, Val Loss: 0.0150\nEpoch 100/100, Train Loss: 0.0140, Val Loss: 0.0150\nEpoch 100 SR min: 0.0000, max: 1.0000\nTrain_Stage4 MSE: 0.0011, SSIM: 0.8663, PSNR: 29.5007\nVal_Stage4 MSE: 0.0007, SSIM: 0.8321, PSNR: 31.3414\nTest_Stage4 MSE: 0.0011, SSIM: 0.8059, PSNR: 29.6637\nFinal fine-tuned model saved as edsr_finetuned_stage4.pth\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Stage 5: Switch to RAdam, Add Denoising, More Epochs (With Augmentation)\n# - Continue using augmentation (train_loader_aug_denoise) to maintain generalization.\n# - Add denoising (Gaussian blur, sigma 1.0) to handle real data noise.\n# - Switch to RAdam, increase epochs to 300, bump perceptual weight to 0.7.\n# - Reset to Task 1 model to isolate the effect of these changes.\n# - Expected: PSNR ~30.5 dB, SSIM ~0.84 - best performance so far.\nmodel = EDSR(scale_factor=2, num_blocks=8, channels=64)\nmodel.load_state_dict(torch.load(\"/kaggle/input/edsr-best-pth/edsr_best.pth\", weights_only=True))\nmodel = finetune(model, train_loader_aug_denoise, val_loader, epochs=300, lr=1e-3, use_perceptual=True, use_radam=True, use_scheduler=True, stage_name=\"Stage5\")\nevaluate_split(model, train_loader_aug_denoise, \"Train_Stage5\", num_vis=3)\nevaluate_split(model, val_loader, \"Val_Stage5\", num_vis=3)\nevaluate_split(model, test_loader, \"Test_Stage5\", num_vis=3)\ntorch.save(model.state_dict(), \"edsr_finetuned_stage5.pth\")\nprint(\"Final fine-tuned model saved as edsr_finetuned_stage5.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T15:01:41.223894Z","iopub.execute_input":"2025-03-25T15:01:41.224235Z","iopub.status.idle":"2025-03-25T15:12:32.342604Z","shell.execute_reply.started":"2025-03-25T15:01:41.224206Z","shell.execute_reply":"2025-03-25T15:12:32.341675Z"}},"outputs":[{"name":"stdout","text":"\n=== Stage5 ===\nModel on device: cuda:0\nInitial VRAM usage: 77.69 MB\nLR shape: torch.Size([8, 1, 64, 64]), HR shape: torch.Size([8, 1, 128, 128])\nSR shape: torch.Size([8, 1, 128, 128]), SR min: 0.0000, max: 0.8985\nEpoch 1/300, Train Loss: 0.0397, Val Loss: 0.0216\nEpoch 2/300, Train Loss: 0.0196, Val Loss: 0.0236\nEpoch 3/300, Train Loss: 0.0182, Val Loss: 0.0199\nEpoch 4/300, Train Loss: 0.0164, Val Loss: 0.0207\nEpoch 5/300, Train Loss: 0.0155, Val Loss: 0.0191\nEpoch 5 SR min: 0.0000, max: 1.0000\nEpoch 6/300, Train Loss: 0.0151, Val Loss: 0.0204\nEpoch 7/300, Train Loss: 0.0146, Val Loss: 0.0170\nEpoch 8/300, Train Loss: 0.0144, Val Loss: 0.0171\nEpoch 9/300, Train Loss: 0.0141, Val Loss: 0.0163\nEpoch 10/300, Train Loss: 0.0147, Val Loss: 0.0169\nEpoch 10 SR min: 0.0000, max: 1.0000\nEpoch 11/300, Train Loss: 0.0146, Val Loss: 0.0165\nEpoch 12/300, Train Loss: 0.0140, Val Loss: 0.0166\nEpoch 13/300, Train Loss: 0.0140, Val Loss: 0.0163\nEpoch 14/300, Train Loss: 0.0145, Val Loss: 0.0163\nEpoch 15/300, Train Loss: 0.0140, Val Loss: 0.0163\nEpoch 15 SR min: 0.0000, max: 1.0000\nEpoch 16/300, Train Loss: 0.0137, Val Loss: 0.0160\nEpoch 17/300, Train Loss: 0.0141, Val Loss: 0.0160\nEpoch 18/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 19/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 20/300, Train Loss: 0.0136, Val Loss: 0.0161\nEpoch 20 SR min: 0.0000, max: 1.0000\nEpoch 21/300, Train Loss: 0.0145, Val Loss: 0.0165\nEpoch 22/300, Train Loss: 0.0143, Val Loss: 0.0163\nEpoch 23/300, Train Loss: 0.0146, Val Loss: 0.0173\nEpoch 24/300, Train Loss: 0.0146, Val Loss: 0.0174\nEpoch 25/300, Train Loss: 0.0141, Val Loss: 0.0164\nEpoch 25 SR min: 0.0000, max: 0.9199\nEpoch 26/300, Train Loss: 0.0142, Val Loss: 0.0185\nEpoch 27/300, Train Loss: 0.0149, Val Loss: 0.0165\nEpoch 28/300, Train Loss: 0.0143, Val Loss: 0.0167\nEpoch 29/300, Train Loss: 0.0142, Val Loss: 0.0164\nEpoch 30/300, Train Loss: 0.0141, Val Loss: 0.0167\nEpoch 30 SR min: 0.0000, max: 1.0000\nEpoch 31/300, Train Loss: 0.0140, Val Loss: 0.0161\nEpoch 32/300, Train Loss: 0.0139, Val Loss: 0.0162\nEpoch 33/300, Train Loss: 0.0140, Val Loss: 0.0162\nEpoch 34/300, Train Loss: 0.0139, Val Loss: 0.0161\nEpoch 35/300, Train Loss: 0.0139, Val Loss: 0.0161\nEpoch 35 SR min: 0.0000, max: 0.9900\nEpoch 36/300, Train Loss: 0.0136, Val Loss: 0.0163\nEpoch 37/300, Train Loss: 0.0133, Val Loss: 0.0160\nEpoch 38/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 39/300, Train Loss: 0.0136, Val Loss: 0.0160\nEpoch 40/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 40 SR min: 0.0000, max: 1.0000\nEpoch 41/300, Train Loss: 0.0143, Val Loss: 0.0166\nEpoch 42/300, Train Loss: 0.0147, Val Loss: 0.0166\nEpoch 43/300, Train Loss: 0.0142, Val Loss: 0.0171\nEpoch 44/300, Train Loss: 0.0144, Val Loss: 0.0180\nEpoch 45/300, Train Loss: 0.0145, Val Loss: 0.0164\nEpoch 45 SR min: 0.0000, max: 0.9996\nEpoch 46/300, Train Loss: 0.0144, Val Loss: 0.0174\nEpoch 47/300, Train Loss: 0.0138, Val Loss: 0.0163\nEpoch 48/300, Train Loss: 0.0142, Val Loss: 0.0166\nEpoch 49/300, Train Loss: 0.0147, Val Loss: 0.0179\nEpoch 50/300, Train Loss: 0.0145, Val Loss: 0.0168\nEpoch 50 SR min: 0.0000, max: 1.0000\nEpoch 51/300, Train Loss: 0.0140, Val Loss: 0.0163\nEpoch 52/300, Train Loss: 0.0139, Val Loss: 0.0164\nEpoch 53/300, Train Loss: 0.0137, Val Loss: 0.0161\nEpoch 54/300, Train Loss: 0.0136, Val Loss: 0.0163\nEpoch 55/300, Train Loss: 0.0137, Val Loss: 0.0161\nEpoch 55 SR min: 0.0000, max: 1.0000\nEpoch 56/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 57/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 58/300, Train Loss: 0.0134, Val Loss: 0.0160\nEpoch 59/300, Train Loss: 0.0136, Val Loss: 0.0160\nEpoch 60/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 60 SR min: 0.0000, max: 1.0000\nEpoch 61/300, Train Loss: 0.0147, Val Loss: 0.0171\nEpoch 62/300, Train Loss: 0.0150, Val Loss: 0.0167\nEpoch 63/300, Train Loss: 0.0142, Val Loss: 0.0214\nEpoch 64/300, Train Loss: 0.0152, Val Loss: 0.0173\nEpoch 65/300, Train Loss: 0.0145, Val Loss: 0.0172\nEpoch 65 SR min: 0.0000, max: 1.0000\nEpoch 66/300, Train Loss: 0.0143, Val Loss: 0.0167\nEpoch 67/300, Train Loss: 0.0141, Val Loss: 0.0163\nEpoch 68/300, Train Loss: 0.0142, Val Loss: 0.0164\nEpoch 69/300, Train Loss: 0.0138, Val Loss: 0.0160\nEpoch 70/300, Train Loss: 0.0140, Val Loss: 0.0164\nEpoch 70 SR min: 0.0000, max: 1.0000\nEpoch 71/300, Train Loss: 0.0139, Val Loss: 0.0166\nEpoch 72/300, Train Loss: 0.0140, Val Loss: 0.0162\nEpoch 73/300, Train Loss: 0.0136, Val Loss: 0.0160\nEpoch 74/300, Train Loss: 0.0139, Val Loss: 0.0163\nEpoch 75/300, Train Loss: 0.0137, Val Loss: 0.0163\nEpoch 75 SR min: 0.0000, max: 1.0000\nEpoch 76/300, Train Loss: 0.0136, Val Loss: 0.0160\nEpoch 77/300, Train Loss: 0.0136, Val Loss: 0.0163\nEpoch 78/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 79/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 80/300, Train Loss: 0.0136, Val Loss: 0.0161\nEpoch 80 SR min: 0.0000, max: 1.0000\nEpoch 81/300, Train Loss: 0.0141, Val Loss: 0.0165\nEpoch 82/300, Train Loss: 0.0146, Val Loss: 0.0163\nEpoch 83/300, Train Loss: 0.0147, Val Loss: 0.0171\nEpoch 84/300, Train Loss: 0.0142, Val Loss: 0.0171\nEpoch 85/300, Train Loss: 0.0145, Val Loss: 0.0170\nEpoch 85 SR min: 0.0000, max: 1.0000\nEpoch 86/300, Train Loss: 0.0143, Val Loss: 0.0167\nEpoch 87/300, Train Loss: 0.0144, Val Loss: 0.0165\nEpoch 88/300, Train Loss: 0.0144, Val Loss: 0.0173\nEpoch 89/300, Train Loss: 0.0141, Val Loss: 0.0167\nEpoch 90/300, Train Loss: 0.0142, Val Loss: 0.0169\nEpoch 90 SR min: 0.0000, max: 1.0000\nEpoch 91/300, Train Loss: 0.0141, Val Loss: 0.0166\nEpoch 92/300, Train Loss: 0.0138, Val Loss: 0.0161\nEpoch 93/300, Train Loss: 0.0140, Val Loss: 0.0163\nEpoch 94/300, Train Loss: 0.0137, Val Loss: 0.0165\nEpoch 95/300, Train Loss: 0.0137, Val Loss: 0.0163\nEpoch 95 SR min: 0.0000, max: 1.0000\nEpoch 96/300, Train Loss: 0.0138, Val Loss: 0.0161\nEpoch 97/300, Train Loss: 0.0138, Val Loss: 0.0161\nEpoch 98/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 99/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 100/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 100 SR min: 0.0000, max: 1.0000\nEpoch 101/300, Train Loss: 0.0147, Val Loss: 0.0166\nEpoch 102/300, Train Loss: 0.0142, Val Loss: 0.0163\nEpoch 103/300, Train Loss: 0.0140, Val Loss: 0.0166\nEpoch 104/300, Train Loss: 0.0144, Val Loss: 0.0162\nEpoch 105/300, Train Loss: 0.0140, Val Loss: 0.0163\nEpoch 105 SR min: 0.0000, max: 1.0000\nEpoch 106/300, Train Loss: 0.0143, Val Loss: 0.0161\nEpoch 107/300, Train Loss: 0.0143, Val Loss: 0.0175\nEpoch 108/300, Train Loss: 0.0140, Val Loss: 0.0170\nEpoch 109/300, Train Loss: 0.0141, Val Loss: 0.0164\nEpoch 110/300, Train Loss: 0.0137, Val Loss: 0.0164\nEpoch 110 SR min: 0.0000, max: 1.0000\nEpoch 111/300, Train Loss: 0.0144, Val Loss: 0.0165\nEpoch 112/300, Train Loss: 0.0140, Val Loss: 0.0163\nEpoch 113/300, Train Loss: 0.0139, Val Loss: 0.0160\nEpoch 114/300, Train Loss: 0.0139, Val Loss: 0.0160\nEpoch 115/300, Train Loss: 0.0137, Val Loss: 0.0162\nEpoch 115 SR min: 0.0000, max: 1.0000\nEpoch 116/300, Train Loss: 0.0139, Val Loss: 0.0162\nEpoch 117/300, Train Loss: 0.0137, Val Loss: 0.0161\nEpoch 118/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 119/300, Train Loss: 0.0136, Val Loss: 0.0161\nEpoch 120/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 120 SR min: 0.0000, max: 1.0000\nEpoch 121/300, Train Loss: 0.0140, Val Loss: 0.0161\nEpoch 122/300, Train Loss: 0.0142, Val Loss: 0.0172\nEpoch 123/300, Train Loss: 0.0140, Val Loss: 0.0162\nEpoch 124/300, Train Loss: 0.0138, Val Loss: 0.0176\nEpoch 125/300, Train Loss: 0.0142, Val Loss: 0.0163\nEpoch 125 SR min: 0.0000, max: 1.0000\nEpoch 126/300, Train Loss: 0.0141, Val Loss: 0.0163\nEpoch 127/300, Train Loss: 0.0142, Val Loss: 0.0179\nEpoch 128/300, Train Loss: 0.0149, Val Loss: 0.0170\nEpoch 129/300, Train Loss: 0.0142, Val Loss: 0.0167\nEpoch 130/300, Train Loss: 0.0138, Val Loss: 0.0160\nEpoch 130 SR min: 0.0000, max: 1.0000\nEpoch 131/300, Train Loss: 0.0140, Val Loss: 0.0165\nEpoch 132/300, Train Loss: 0.0136, Val Loss: 0.0161\nEpoch 133/300, Train Loss: 0.0138, Val Loss: 0.0170\nEpoch 134/300, Train Loss: 0.0135, Val Loss: 0.0163\nEpoch 135/300, Train Loss: 0.0137, Val Loss: 0.0161\nEpoch 135 SR min: 0.0000, max: 1.0000\nEpoch 136/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 137/300, Train Loss: 0.0136, Val Loss: 0.0161\nEpoch 138/300, Train Loss: 0.0134, Val Loss: 0.0162\nEpoch 139/300, Train Loss: 0.0134, Val Loss: 0.0162\nEpoch 140/300, Train Loss: 0.0134, Val Loss: 0.0162\nEpoch 140 SR min: 0.0000, max: 1.0000\nEpoch 141/300, Train Loss: 0.0142, Val Loss: 0.0201\nEpoch 142/300, Train Loss: 0.0143, Val Loss: 0.0167\nEpoch 143/300, Train Loss: 0.0143, Val Loss: 0.0178\nEpoch 144/300, Train Loss: 0.0146, Val Loss: 0.0167\nEpoch 145/300, Train Loss: 0.0147, Val Loss: 0.0168\nEpoch 145 SR min: 0.0000, max: 1.0000\nEpoch 146/300, Train Loss: 0.0141, Val Loss: 0.0168\nEpoch 147/300, Train Loss: 0.0141, Val Loss: 0.0164\nEpoch 148/300, Train Loss: 0.0137, Val Loss: 0.0162\nEpoch 149/300, Train Loss: 0.0138, Val Loss: 0.0170\nEpoch 150/300, Train Loss: 0.0142, Val Loss: 0.0176\nEpoch 150 SR min: 0.0000, max: 1.0000\nEpoch 151/300, Train Loss: 0.0138, Val Loss: 0.0162\nEpoch 152/300, Train Loss: 0.0139, Val Loss: 0.0162\nEpoch 153/300, Train Loss: 0.0137, Val Loss: 0.0159\nEpoch 154/300, Train Loss: 0.0137, Val Loss: 0.0163\nEpoch 155/300, Train Loss: 0.0138, Val Loss: 0.0160\nEpoch 155 SR min: 0.0000, max: 1.0000\nEpoch 156/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 157/300, Train Loss: 0.0136, Val Loss: 0.0159\nEpoch 158/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 159/300, Train Loss: 0.0136, Val Loss: 0.0160\nEpoch 160/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 160 SR min: 0.0000, max: 1.0000\nEpoch 161/300, Train Loss: 0.0142, Val Loss: 0.0166\nEpoch 162/300, Train Loss: 0.0139, Val Loss: 0.0160\nEpoch 163/300, Train Loss: 0.0139, Val Loss: 0.0167\nEpoch 164/300, Train Loss: 0.0139, Val Loss: 0.0164\nEpoch 165/300, Train Loss: 0.0144, Val Loss: 0.0163\nEpoch 165 SR min: 0.0000, max: 1.0000\nEpoch 166/300, Train Loss: 0.0142, Val Loss: 0.0159\nEpoch 167/300, Train Loss: 0.0141, Val Loss: 0.0163\nEpoch 168/300, Train Loss: 0.0140, Val Loss: 0.0165\nEpoch 169/300, Train Loss: 0.0145, Val Loss: 0.0164\nEpoch 170/300, Train Loss: 0.0139, Val Loss: 0.0162\nEpoch 170 SR min: 0.0000, max: 1.0000\nEpoch 171/300, Train Loss: 0.0137, Val Loss: 0.0160\nEpoch 172/300, Train Loss: 0.0136, Val Loss: 0.0159\nEpoch 173/300, Train Loss: 0.0139, Val Loss: 0.0161\nEpoch 174/300, Train Loss: 0.0136, Val Loss: 0.0159\nEpoch 175/300, Train Loss: 0.0138, Val Loss: 0.0160\nEpoch 175 SR min: 0.0000, max: 1.0000\nEpoch 176/300, Train Loss: 0.0136, Val Loss: 0.0162\nEpoch 177/300, Train Loss: 0.0135, Val Loss: 0.0162\nEpoch 178/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 179/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 180/300, Train Loss: 0.0136, Val Loss: 0.0161\nEpoch 180 SR min: 0.0000, max: 1.0000\nEpoch 181/300, Train Loss: 0.0139, Val Loss: 0.0162\nEpoch 182/300, Train Loss: 0.0139, Val Loss: 0.0166\nEpoch 183/300, Train Loss: 0.0142, Val Loss: 0.0168\nEpoch 184/300, Train Loss: 0.0141, Val Loss: 0.0167\nEpoch 185/300, Train Loss: 0.0143, Val Loss: 0.0164\nEpoch 185 SR min: 0.0000, max: 1.0000\nEpoch 186/300, Train Loss: 0.0143, Val Loss: 0.0166\nEpoch 187/300, Train Loss: 0.0137, Val Loss: 0.0162\nEpoch 188/300, Train Loss: 0.0137, Val Loss: 0.0162\nEpoch 189/300, Train Loss: 0.0137, Val Loss: 0.0162\nEpoch 190/300, Train Loss: 0.0138, Val Loss: 0.0163\nEpoch 190 SR min: 0.0000, max: 1.0000\nEpoch 191/300, Train Loss: 0.0136, Val Loss: 0.0163\nEpoch 192/300, Train Loss: 0.0138, Val Loss: 0.0168\nEpoch 193/300, Train Loss: 0.0137, Val Loss: 0.0161\nEpoch 194/300, Train Loss: 0.0139, Val Loss: 0.0162\nEpoch 195/300, Train Loss: 0.0136, Val Loss: 0.0162\nEpoch 195 SR min: 0.0000, max: 1.0000\nEpoch 196/300, Train Loss: 0.0138, Val Loss: 0.0162\nEpoch 197/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 198/300, Train Loss: 0.0136, Val Loss: 0.0160\nEpoch 199/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 200/300, Train Loss: 0.0136, Val Loss: 0.0161\nEpoch 200 SR min: 0.0000, max: 1.0000\nEpoch 201/300, Train Loss: 0.0138, Val Loss: 0.0166\nEpoch 202/300, Train Loss: 0.0141, Val Loss: 0.0186\nEpoch 203/300, Train Loss: 0.0143, Val Loss: 0.0163\nEpoch 204/300, Train Loss: 0.0143, Val Loss: 0.0164\nEpoch 205/300, Train Loss: 0.0142, Val Loss: 0.0174\nEpoch 205 SR min: 0.0000, max: 1.0000\nEpoch 206/300, Train Loss: 0.0144, Val Loss: 0.0163\nEpoch 207/300, Train Loss: 0.0142, Val Loss: 0.0164\nEpoch 208/300, Train Loss: 0.0139, Val Loss: 0.0161\nEpoch 209/300, Train Loss: 0.0139, Val Loss: 0.0160\nEpoch 210/300, Train Loss: 0.0140, Val Loss: 0.0163\nEpoch 210 SR min: 0.0000, max: 1.0000\nEpoch 211/300, Train Loss: 0.0137, Val Loss: 0.0163\nEpoch 212/300, Train Loss: 0.0139, Val Loss: 0.0163\nEpoch 213/300, Train Loss: 0.0137, Val Loss: 0.0162\nEpoch 214/300, Train Loss: 0.0137, Val Loss: 0.0161\nEpoch 215/300, Train Loss: 0.0138, Val Loss: 0.0161\nEpoch 215 SR min: 0.0000, max: 1.0000\nEpoch 216/300, Train Loss: 0.0134, Val Loss: 0.0161\nEpoch 217/300, Train Loss: 0.0134, Val Loss: 0.0161\nEpoch 218/300, Train Loss: 0.0134, Val Loss: 0.0160\nEpoch 219/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 220/300, Train Loss: 0.0134, Val Loss: 0.0160\nEpoch 220 SR min: 0.0000, max: 1.0000\nEpoch 221/300, Train Loss: 0.0139, Val Loss: 0.0161\nEpoch 222/300, Train Loss: 0.0140, Val Loss: 0.0163\nEpoch 223/300, Train Loss: 0.0144, Val Loss: 0.0161\nEpoch 224/300, Train Loss: 0.0141, Val Loss: 0.0163\nEpoch 225/300, Train Loss: 0.0141, Val Loss: 0.0163\nEpoch 225 SR min: 0.0000, max: 1.0000\nEpoch 226/300, Train Loss: 0.0140, Val Loss: 0.0172\nEpoch 227/300, Train Loss: 0.0144, Val Loss: 0.0164\nEpoch 228/300, Train Loss: 0.0138, Val Loss: 0.0164\nEpoch 229/300, Train Loss: 0.0138, Val Loss: 0.0161\nEpoch 230/300, Train Loss: 0.0141, Val Loss: 0.0163\nEpoch 230 SR min: 0.0000, max: 1.0000\nEpoch 231/300, Train Loss: 0.0141, Val Loss: 0.0160\nEpoch 232/300, Train Loss: 0.0137, Val Loss: 0.0161\nEpoch 233/300, Train Loss: 0.0136, Val Loss: 0.0162\nEpoch 234/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 235/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 235 SR min: 0.0000, max: 1.0000\nEpoch 236/300, Train Loss: 0.0136, Val Loss: 0.0160\nEpoch 237/300, Train Loss: 0.0137, Val Loss: 0.0160\nEpoch 238/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 239/300, Train Loss: 0.0134, Val Loss: 0.0160\nEpoch 240/300, Train Loss: 0.0136, Val Loss: 0.0160\nEpoch 240 SR min: 0.0000, max: 1.0000\nEpoch 241/300, Train Loss: 0.0139, Val Loss: 0.0163\nEpoch 242/300, Train Loss: 0.0142, Val Loss: 0.0163\nEpoch 243/300, Train Loss: 0.0141, Val Loss: 0.0169\nEpoch 244/300, Train Loss: 0.0139, Val Loss: 0.0166\nEpoch 245/300, Train Loss: 0.0140, Val Loss: 0.0164\nEpoch 245 SR min: 0.0000, max: 1.0000\nEpoch 246/300, Train Loss: 0.0141, Val Loss: 0.0161\nEpoch 247/300, Train Loss: 0.0140, Val Loss: 0.0168\nEpoch 248/300, Train Loss: 0.0140, Val Loss: 0.0164\nEpoch 249/300, Train Loss: 0.0137, Val Loss: 0.0160\nEpoch 250/300, Train Loss: 0.0140, Val Loss: 0.0162\nEpoch 250 SR min: 0.0000, max: 1.0000\nEpoch 251/300, Train Loss: 0.0136, Val Loss: 0.0160\nEpoch 252/300, Train Loss: 0.0138, Val Loss: 0.0162\nEpoch 253/300, Train Loss: 0.0137, Val Loss: 0.0162\nEpoch 254/300, Train Loss: 0.0136, Val Loss: 0.0163\nEpoch 255/300, Train Loss: 0.0137, Val Loss: 0.0161\nEpoch 255 SR min: 0.0000, max: 1.0000\nEpoch 256/300, Train Loss: 0.0134, Val Loss: 0.0160\nEpoch 257/300, Train Loss: 0.0136, Val Loss: 0.0161\nEpoch 258/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 259/300, Train Loss: 0.0134, Val Loss: 0.0160\nEpoch 260/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 260 SR min: 0.0000, max: 1.0000\nEpoch 261/300, Train Loss: 0.0140, Val Loss: 0.0162\nEpoch 262/300, Train Loss: 0.0140, Val Loss: 0.0164\nEpoch 263/300, Train Loss: 0.0137, Val Loss: 0.0164\nEpoch 264/300, Train Loss: 0.0138, Val Loss: 0.0159\nEpoch 265/300, Train Loss: 0.0144, Val Loss: 0.0162\nEpoch 265 SR min: 0.0000, max: 1.0000\nEpoch 266/300, Train Loss: 0.0139, Val Loss: 0.0169\nEpoch 267/300, Train Loss: 0.0139, Val Loss: 0.0161\nEpoch 268/300, Train Loss: 0.0138, Val Loss: 0.0161\nEpoch 269/300, Train Loss: 0.0142, Val Loss: 0.0163\nEpoch 270/300, Train Loss: 0.0137, Val Loss: 0.0162\nEpoch 270 SR min: 0.0000, max: 1.0000\nEpoch 271/300, Train Loss: 0.0139, Val Loss: 0.0160\nEpoch 272/300, Train Loss: 0.0138, Val Loss: 0.0160\nEpoch 273/300, Train Loss: 0.0137, Val Loss: 0.0161\nEpoch 274/300, Train Loss: 0.0134, Val Loss: 0.0162\nEpoch 275/300, Train Loss: 0.0137, Val Loss: 0.0160\nEpoch 275 SR min: 0.0000, max: 1.0000\nEpoch 276/300, Train Loss: 0.0137, Val Loss: 0.0161\nEpoch 277/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 278/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 279/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 280/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 280 SR min: 0.0000, max: 1.0000\nEpoch 281/300, Train Loss: 0.0140, Val Loss: 0.0166\nEpoch 282/300, Train Loss: 0.0143, Val Loss: 0.0163\nEpoch 283/300, Train Loss: 0.0140, Val Loss: 0.0164\nEpoch 284/300, Train Loss: 0.0139, Val Loss: 0.0169\nEpoch 285/300, Train Loss: 0.0141, Val Loss: 0.0172\nEpoch 285 SR min: 0.0000, max: 1.0000\nEpoch 286/300, Train Loss: 0.0139, Val Loss: 0.0172\nEpoch 287/300, Train Loss: 0.0142, Val Loss: 0.0160\nEpoch 288/300, Train Loss: 0.0137, Val Loss: 0.0162\nEpoch 289/300, Train Loss: 0.0141, Val Loss: 0.0163\nEpoch 290/300, Train Loss: 0.0137, Val Loss: 0.0163\nEpoch 290 SR min: 0.0000, max: 1.0000\nEpoch 291/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 292/300, Train Loss: 0.0137, Val Loss: 0.0164\nEpoch 293/300, Train Loss: 0.0136, Val Loss: 0.0160\nEpoch 294/300, Train Loss: 0.0136, Val Loss: 0.0159\nEpoch 295/300, Train Loss: 0.0135, Val Loss: 0.0161\nEpoch 295 SR min: 0.0000, max: 1.0000\nEpoch 296/300, Train Loss: 0.0134, Val Loss: 0.0162\nEpoch 297/300, Train Loss: 0.0134, Val Loss: 0.0160\nEpoch 298/300, Train Loss: 0.0134, Val Loss: 0.0160\nEpoch 299/300, Train Loss: 0.0135, Val Loss: 0.0160\nEpoch 300/300, Train Loss: 0.0134, Val Loss: 0.0160\nEpoch 300 SR min: 0.0000, max: 1.0000\nTrain_Stage5 MSE: 0.0011, SSIM: 0.8858, PSNR: 29.7367\nVal_Stage5 MSE: 0.0008, SSIM: 0.8082, PSNR: 30.8990\nTest_Stage5 MSE: 0.0011, SSIM: 0.7913, PSNR: 29.4418\nFinal fine-tuned model saved as edsr_finetuned_stage5.pth\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"best_model = EDSR(scale_factor=2, num_blocks=8, channels=64)\nbest_model.load_state_dict(torch.load(\"/kaggle/input/edsr_finetuned_best_stage1/pytorch/default/1/edsr_finetuned_best_Stage1.pth\", weights_only=True))\nevaluate_split(best_model, train_loader_aug, \"Train_Stage1\", num_vis=3)\nevaluate_split(best_model, val_loader, \"Val_Stage1\", num_vis=3)\nevaluate_split(best_model, test_loader, \"Test_Stage1\", num_vis=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T15:26:49.270175Z","iopub.execute_input":"2025-03-25T15:26:49.270531Z","iopub.status.idle":"2025-03-25T15:26:53.523146Z","shell.execute_reply.started":"2025-03-25T15:26:49.270503Z","shell.execute_reply":"2025-03-25T15:26:53.522269Z"}},"outputs":[{"name":"stdout","text":"Train_Stage1 MSE: 0.0017, SSIM: 0.8576, PSNR: 27.6013\nVal_Stage1 MSE: 0.0013, SSIM: 0.8381, PSNR: 28.7617\nTest_Stage1 MSE: 0.0016, SSIM: 0.8064, PSNR: 27.9314\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"best_model = EDSR(scale_factor=2, num_blocks=8, channels=64)\nbest_model.load_state_dict(torch.load(\"/kaggle/input/edsr_finetuned_best_stage2/pytorch/default/1/edsr_finetuned_best_Stage2.pth\", weights_only=True))\nevaluate_split(best_model, train_loader_aug, \"Train_Stage2\", num_vis=3)\nevaluate_split(best_model, val_loader, \"Val_Stage2\", num_vis=3)\nevaluate_split(best_model, test_loader, \"Test_Stage2\", num_vis=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T15:27:30.097292Z","iopub.execute_input":"2025-03-25T15:27:30.097634Z","iopub.status.idle":"2025-03-25T15:27:34.602053Z","shell.execute_reply.started":"2025-03-25T15:27:30.097607Z","shell.execute_reply":"2025-03-25T15:27:34.600996Z"}},"outputs":[{"name":"stdout","text":"Train_Stage2 MSE: 0.0016, SSIM: 0.8548, PSNR: 27.8580\nVal_Stage2 MSE: 0.0012, SSIM: 0.8324, PSNR: 29.0457\nTest_Stage2 MSE: 0.0015, SSIM: 0.8028, PSNR: 28.1744\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"best_model = EDSR(scale_factor=2, num_blocks=8, channels=64)\nbest_model.load_state_dict(torch.load(\"/kaggle/input/edsr_finetuned_best_stage3/pytorch/default/1/edsr_finetuned_best_Stage3.pth\", weights_only=True))\nevaluate_split(best_model, train_loader_aug, \"Train_Stage3\", num_vis=3)\nevaluate_split(best_model, val_loader, \"Val_Stage3\", num_vis=3)\nevaluate_split(best_model, test_loader, \"Test_Stage3\", num_vis=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T15:28:09.506309Z","iopub.execute_input":"2025-03-25T15:28:09.506648Z","iopub.status.idle":"2025-03-25T15:28:13.944641Z","shell.execute_reply.started":"2025-03-25T15:28:09.506620Z","shell.execute_reply":"2025-03-25T15:28:13.943810Z"}},"outputs":[{"name":"stdout","text":"Train_Stage3 MSE: 0.0011, SSIM: 0.8572, PSNR: 29.4790\nVal_Stage3 MSE: 0.0007, SSIM: 0.8296, PSNR: 31.6401\nTest_Stage3 MSE: 0.0010, SSIM: 0.8027, PSNR: 29.7925\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"best_model = EDSR(scale_factor=2, num_blocks=8, channels=64)\nbest_model.load_state_dict(torch.load(\"/kaggle/input/edsr_finetuned_best_stage4/pytorch/default/1/edsr_finetuned_best_Stage4.pth\", weights_only=True))\nevaluate_split(best_model, train_loader_aug, \"Train_Stage4\", num_vis=3)\nevaluate_split(best_model, val_loader, \"Val_Stage4\", num_vis=3)\nevaluate_split(best_model, test_loader, \"Test_Stage4\", num_vis=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T15:28:24.456253Z","iopub.execute_input":"2025-03-25T15:28:24.456552Z","iopub.status.idle":"2025-03-25T15:28:28.583291Z","shell.execute_reply.started":"2025-03-25T15:28:24.456518Z","shell.execute_reply":"2025-03-25T15:28:28.582298Z"}},"outputs":[{"name":"stdout","text":"Train_Stage4 MSE: 0.0012, SSIM: 0.8641, PSNR: 29.2611\nVal_Stage4 MSE: 0.0007, SSIM: 0.8324, PSNR: 31.5415\nTest_Stage4 MSE: 0.0011, SSIM: 0.8064, PSNR: 29.7422\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"best_model = EDSR(scale_factor=2, num_blocks=8, channels=64)\nbest_model.load_state_dict(torch.load(\"/kaggle/input/edsr_finetuned_best_stage5/pytorch/default/1/edsr_finetuned_best_Stage5.pth\", weights_only=True))\nevaluate_split(best_model, train_loader_aug, \"Train_Stage5\", num_vis=3)\nevaluate_split(best_model, val_loader, \"Val_Stage5\", num_vis=3)\nevaluate_split(best_model, test_loader, \"Test_Stage5\", num_vis=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T15:28:52.676256Z","iopub.execute_input":"2025-03-25T15:28:52.676631Z","iopub.status.idle":"2025-03-25T15:28:57.135475Z","shell.execute_reply.started":"2025-03-25T15:28:52.676598Z","shell.execute_reply":"2025-03-25T15:28:57.134391Z"}},"outputs":[{"name":"stdout","text":"Train_Stage5 MSE: 0.0012, SSIM: 0.8505, PSNR: 29.2129\nVal_Stage5 MSE: 0.0008, SSIM: 0.8050, PSNR: 31.0698\nTest_Stage5 MSE: 0.0011, SSIM: 0.7889, PSNR: 29.4955\n","output_type":"stream"}],"execution_count":54}]}