{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11063164,"sourceType":"datasetVersion","datasetId":6893445}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom timm import create_model\nfrom itertools import product\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\nfrom torchvision.models import resnet50\nimport matplotlib.pyplot as plt\nimport random\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:22:42.851134Z","iopub.execute_input":"2025-03-30T10:22:42.851479Z","iopub.status.idle":"2025-03-30T10:22:42.856102Z","shell.execute_reply.started":"2025-03-30T10:22:42.851452Z","shell.execute_reply":"2025-03-30T10:22:42.855210Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class LensingDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = ['no', 'sphere', 'vort']\n        self.files = []\n        for idx, cls in enumerate(self.classes):\n            cls_dir = os.path.join(root_dir, cls)\n            for f in os.listdir(cls_dir):\n                if f.endswith('.npy'):\n                    self.files.append((os.path.join(cls_dir, f), idx))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        file_path, label = self.files[idx]\n        img = np.load(file_path)  # (1, 150, 150), float64, 0-1\n        img = torch.tensor(img, dtype=torch.float32)\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:22:44.059397Z","iopub.execute_input":"2025-03-30T10:22:44.059688Z","iopub.status.idle":"2025-03-30T10:22:44.065705Z","shell.execute_reply.started":"2025-03-30T10:22:44.059667Z","shell.execute_reply":"2025-03-30T10:22:44.064804Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Modified the ResNet50 for 1-channel input\nclass CustomResNet50(nn.Module):\n    def __init__(self, num_classes=3, dropout_rate=0.0):\n        super().__init__()\n        base_model = resnet50(pretrained=True)\n        # Replace first conv layer (3 channels -> 1 channel)\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        # Copy weights for single channel (mean across RGB)\n        with torch.no_grad():\n            self.conv1.weight = nn.Parameter(base_model.conv1.weight.mean(dim=1, keepdim=True))\n        # Rest of ResNet\n        self.bn1 = base_model.bn1\n        self.relu = base_model.relu\n        self.maxpool = base_model.maxpool\n        self.layer1 = base_model.layer1\n        self.layer2 = base_model.layer2\n        self.layer3 = base_model.layer3\n        self.layer4 = base_model.layer4\n        self.avgpool = base_model.avgpool\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout_rate),\n            nn.Linear(base_model.fc.in_features, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:22:45.947373Z","iopub.execute_input":"2025-03-30T10:22:45.947664Z","iopub.status.idle":"2025-03-30T10:22:45.954417Z","shell.execute_reply.started":"2025-03-30T10:22:45.947644Z","shell.execute_reply":"2025-03-30T10:22:45.953562Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Resized the 150 x 150 images into 224 x 224 (supporting Resnet inputs)\n# Applied Data Augmentation to only training data\n# The images are already min-max normalized (0 to 1), slapping on extra normalization would be an overkill - skipped it.\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1))\n])\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:22:47.535060Z","iopub.execute_input":"2025-03-30T10:22:47.535390Z","iopub.status.idle":"2025-03-30T10:22:47.539987Z","shell.execute_reply.started":"2025-03-30T10:22:47.535362Z","shell.execute_reply":"2025-03-30T10:22:47.539112Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_dataset = LensingDataset('/kaggle/input/deeplense-common-test/dataset/train', transform=train_transform)\nval_dataset = LensingDataset('/kaggle/input/deeplense-common-test/dataset/train', transform=val_transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:22:48.777997Z","iopub.execute_input":"2025-03-30T10:22:48.778369Z","iopub.status.idle":"2025-03-30T10:22:49.140254Z","shell.execute_reply.started":"2025-03-30T10:22:48.778340Z","shell.execute_reply":"2025-03-30T10:22:49.139371Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"learning_rates = [1e-5, 3e-5, 1e-4, 3e-4]\nbatch_sizes = [16, 32, 64]\ndrop_rates = [0.0, 0.1, 0.2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:22:50.138865Z","iopub.execute_input":"2025-03-30T10:22:50.139164Z","iopub.status.idle":"2025-03-30T10:22:50.143593Z","shell.execute_reply.started":"2025-03-30T10:22:50.139140Z","shell.execute_reply":"2025-03-30T10:22:50.142574Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Initially tried to run a grid search over 36 combos (4 LRs × 3 BSs × 3 DRs), training each for 5 epochs to gauge val loss.\n# Due to Time and Computational constrains, switched to Random search over 10 combos (not 36), 3 epochs each (not 5).\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbest_val_loss = float('inf')\nbest_params = None\ntrials = 10\nstart_tuning = time.time()  \n\nfor _ in range(trials):\n    lr = random.choice(learning_rates)\n    bs = random.choice(batch_sizes)\n    dr = random.choice(drop_rates)\n    print(f'\\nTuning: LR={lr}, BS={bs}, DR={dr}')\n    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=4)\n\n    model = CustomResNet50(num_classes=3, dropout_rate=dr).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3)\n\n    for epoch in range(3):\n        model.train()\n        train_loss = 0\n        for imgs, labels in train_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                imgs, labels = imgs.to(device), labels.to(device)\n                outputs = model(imgs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        val_loss /= len(val_loader)\n\n        print(f'Epoch {epoch+1}/3, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n        scheduler.step()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_params = (lr, bs, dr)\n\n\nend_tuning = time.time()  \ntuning_time = end_tuning - start_tuning\nprint(f'\\nBest Params: LR={best_params[0]}, BS={best_params[1]}, DR={best_params[2]}, Val Loss={best_val_loss:.4f}')\nprint(f'Tuning completed in {tuning_time:.2f} seconds ({tuning_time/60:.2f} minutes)\\n')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:22:55.181641Z","iopub.execute_input":"2025-03-30T10:22:55.181966Z","iopub.status.idle":"2025-03-30T13:34:44.339528Z","shell.execute_reply.started":"2025-03-30T10:22:55.181938Z","shell.execute_reply":"2025-03-30T13:34:44.338574Z"}},"outputs":[{"name":"stdout","text":"\nTuning: LR=0.0003, BS=32, DR=0.2\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 169MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Train Loss: 1.0567, Val Loss: 0.9540\nEpoch 2/3, Train Loss: 0.5756, Val Loss: 0.4656\nEpoch 3/3, Train Loss: 0.3458, Val Loss: 0.2602\n\nTuning: LR=0.0001, BS=32, DR=0.0\nEpoch 1/3, Train Loss: 0.7702, Val Loss: 0.3861\nEpoch 2/3, Train Loss: 0.3848, Val Loss: 0.3038\nEpoch 3/3, Train Loss: 0.2726, Val Loss: 0.1973\n\nTuning: LR=0.0001, BS=32, DR=0.2\nEpoch 1/3, Train Loss: 0.7825, Val Loss: 0.4112\nEpoch 2/3, Train Loss: 0.3905, Val Loss: 0.3116\nEpoch 3/3, Train Loss: 0.2819, Val Loss: 0.1990\n\nTuning: LR=0.0003, BS=64, DR=0.2\nEpoch 1/3, Train Loss: 1.0533, Val Loss: 0.9146\nEpoch 2/3, Train Loss: 0.5324, Val Loss: 0.5246\nEpoch 3/3, Train Loss: 0.3269, Val Loss: 0.2466\n\nTuning: LR=0.0001, BS=16, DR=0.2\nEpoch 1/3, Train Loss: 0.8888, Val Loss: 0.4484\nEpoch 2/3, Train Loss: 0.4407, Val Loss: 0.3063\nEpoch 3/3, Train Loss: 0.2925, Val Loss: 0.2256\n\nTuning: LR=1e-05, BS=64, DR=0.1\nEpoch 1/3, Train Loss: 1.0826, Val Loss: 1.0466\nEpoch 2/3, Train Loss: 0.9792, Val Loss: 0.9097\nEpoch 3/3, Train Loss: 0.8835, Val Loss: 0.8610\n\nTuning: LR=0.0001, BS=16, DR=0.0\nEpoch 1/3, Train Loss: 0.7987, Val Loss: 0.4049\nEpoch 2/3, Train Loss: 0.4039, Val Loss: 0.3053\nEpoch 3/3, Train Loss: 0.2734, Val Loss: 0.2304\n\nTuning: LR=0.0003, BS=16, DR=0.2\nEpoch 1/3, Train Loss: 1.1100, Val Loss: 1.1043\nEpoch 2/3, Train Loss: 1.1025, Val Loss: 1.1556\nEpoch 3/3, Train Loss: 1.1001, Val Loss: 1.0992\n\nTuning: LR=3e-05, BS=64, DR=0.2\nEpoch 1/3, Train Loss: 1.0049, Val Loss: 0.7748\nEpoch 2/3, Train Loss: 0.6522, Val Loss: 0.5090\nEpoch 3/3, Train Loss: 0.4926, Val Loss: 0.4197\n\nTuning: LR=0.0001, BS=64, DR=0.2\nEpoch 1/3, Train Loss: 0.8399, Val Loss: 0.4922\nEpoch 2/3, Train Loss: 0.4109, Val Loss: 0.3006\nEpoch 3/3, Train Loss: 0.2942, Val Loss: 0.2315\n\nBest Params: LR=0.0001, BS=32, DR=0.0, Val Loss=0.1973\nTuning completed in 11509.15 seconds (191.82 minutes)\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=best_params[1], shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=best_params[1], shuffle=False, num_workers=4)\nmodel = CustomResNet50(num_classes=3, dropout_rate=best_params[2]).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=best_params[0])\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n\nstart_training = time.time()  \n\nfor epoch in range(15):\n    model.train()\n    train_loss = 0\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(train_loader)\n\n    model.eval()\n    val_loss = 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            preds = torch.softmax(outputs, dim=1)\n            all_preds.append(preds.cpu().numpy())\n            all_labels.append(labels.cpu().numpy())\n    val_loss /= len(val_loader)\n\n    print(f'Final Epoch {epoch+1}/15, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n    scheduler.step\n\nend_training = time.time()  \ntraining_time = end_training - start_training\n\nprint(f'Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:44:51.837982Z","iopub.execute_input":"2025-03-30T13:44:51.838352Z","iopub.status.idle":"2025-03-30T15:20:16.693120Z","shell.execute_reply.started":"2025-03-30T13:44:51.838313Z","shell.execute_reply":"2025-03-30T15:20:16.692076Z"}},"outputs":[{"name":"stdout","text":"Final Epoch 1/15, Train Loss: 0.8281, Val Loss: 0.5354\nFinal Epoch 2/15, Train Loss: 0.4250, Val Loss: 0.3714\nFinal Epoch 3/15, Train Loss: 0.3402, Val Loss: 0.2644\nFinal Epoch 4/15, Train Loss: 0.2930, Val Loss: 0.2392\nFinal Epoch 5/15, Train Loss: 0.2675, Val Loss: 0.2613\nFinal Epoch 6/15, Train Loss: 0.2442, Val Loss: 0.2705\nFinal Epoch 7/15, Train Loss: 0.2316, Val Loss: 0.1764\nFinal Epoch 8/15, Train Loss: 0.2205, Val Loss: 0.1647\nFinal Epoch 9/15, Train Loss: 0.2084, Val Loss: 0.1410\nFinal Epoch 10/15, Train Loss: 0.1975, Val Loss: 0.1566\nFinal Epoch 11/15, Train Loss: 0.1935, Val Loss: 0.1374\nFinal Epoch 12/15, Train Loss: 0.1841, Val Loss: 0.1320\nFinal Epoch 13/15, Train Loss: 0.1787, Val Loss: 0.1348\nFinal Epoch 14/15, Train Loss: 0.1705, Val Loss: 0.1288\nFinal Epoch 15/15, Train Loss: 0.1668, Val Loss: 0.1535\nTraining completed in 5724.37 seconds (95.41 minutes)\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"torch.save(model.state_dict(), \"resnet_model.pth\")\nprint(\"Model saved as resnet_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T15:22:42.391554Z","iopub.execute_input":"2025-03-30T15:22:42.391868Z","iopub.status.idle":"2025-03-30T15:22:42.543957Z","shell.execute_reply.started":"2025-03-30T15:22:42.391841Z","shell.execute_reply":"2025-03-30T15:22:42.543253Z"}},"outputs":[{"name":"stdout","text":"Model saved as resnet_model.pth\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"train_losses = [0.8281, 0.4250, 0.3402, 0.2930, 0.2675, 0.2442, 0.2316, 0.2205, 0.2084, 0.1975, 0.1935, 0.1841, 0.1787, 0.1705, 0.1668]\nval_losses = [0.5354, 0.3714, 0.2644, 0.2392, 0.2613, 0.2705, 0.1764, 0.1647, 0.1410, 0.1566, 0.1374, 0.1320, 0.1348, 0.1288, 0.1535]\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, 16), train_losses, label=\"Train Loss\", marker=\"o\")\nplt.plot(range(1, 16), val_losses, label=\"Validation Loss\", marker=\"s\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss Curve\")\nplt.grid(True)\nplt.savefig(\"loss_curve.png\", dpi=300)\nplt.close()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T15:24:23.011745Z","iopub.execute_input":"2025-03-30T15:24:23.012036Z","iopub.status.idle":"2025-03-30T15:24:23.405714Z","shell.execute_reply.started":"2025-03-30T15:24:23.012002Z","shell.execute_reply":"2025-03-30T15:24:23.405029Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"all_preds = np.concatenate(all_preds)\nall_labels = np.concatenate(all_labels)\nclass_names = ['no', 'sphere', 'vort']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T15:24:53.653447Z","iopub.execute_input":"2025-03-30T15:24:53.653742Z","iopub.status.idle":"2025-03-30T15:24:53.661223Z","shell.execute_reply.started":"2025-03-30T15:24:53.653720Z","shell.execute_reply":"2025-03-30T15:24:53.660611Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"auc_scores = [roc_auc_score(all_labels == i, all_preds[:, i]) for i in range(3)]\nprint(f'AUC Scores: {class_names[0]}={auc_scores[0]:.3f}, {class_names[1]}={auc_scores[1]:.3f}, {class_names[2]}={auc_scores[2]:.3f}, Overall={np.mean(auc_scores):.3f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T15:24:56.860882Z","iopub.execute_input":"2025-03-30T15:24:56.861329Z","iopub.status.idle":"2025-03-30T15:24:56.910811Z","shell.execute_reply.started":"2025-03-30T15:24:56.861274Z","shell.execute_reply":"2025-03-30T15:24:56.909885Z"}},"outputs":[{"name":"stdout","text":"AUC Scores: no=0.995, sphere=0.992, vort=0.997, Overall=0.995\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nfor i in range(3):\n    fpr, tpr, _ = roc_curve(all_labels == i, all_preds[:, i])\n    plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {auc_scores[i]:.3f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves (ResNet50, 224x224)')\nplt.legend()\nplt.grid(True)\nplt.savefig('roc_curves_resnet.png')\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T15:25:11.770901Z","iopub.execute_input":"2025-03-30T15:25:11.771213Z","iopub.status.idle":"2025-03-30T15:25:11.947463Z","shell.execute_reply.started":"2025-03-30T15:25:11.771189Z","shell.execute_reply":"2025-03-30T15:25:11.946638Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"cm = confusion_matrix(all_labels, all_preds.argmax(axis=1))\nprint('Confusion Matrix:')\nprint(cm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T15:25:14.196021Z","iopub.execute_input":"2025-03-30T15:25:14.196441Z","iopub.status.idle":"2025-03-30T15:25:14.203471Z","shell.execute_reply.started":"2025-03-30T15:25:14.196397Z","shell.execute_reply":"2025-03-30T15:25:14.202626Z"}},"outputs":[{"name":"stdout","text":"Confusion Matrix:\n[[9992    3    5]\n [ 813 9013  174]\n [ 401  101 9498]]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nplt.imshow(cm, interpolation='nearest', cmap='Blues')\nplt.title('Confusion Matrix (ResNet50, 224x224)')\nplt.colorbar()\ntick_marks = np.arange(3)\nplt.xticks(tick_marks, class_names, rotation=45)\nplt.yticks(tick_marks, class_names)\nfor i in range(3):\n    for j in range(3):\n        plt.text(j, i, cm[i, j], ha='center', va='center', color='black' if cm[i, j] < cm.max() / 2 else 'white')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.tight_layout()\nplt.savefig('confusion_matrix_resnet.png')\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T15:25:18.144422Z","iopub.execute_input":"2025-03-30T15:25:18.144734Z","iopub.status.idle":"2025-03-30T15:25:18.408633Z","shell.execute_reply.started":"2025-03-30T15:25:18.144710Z","shell.execute_reply":"2025-03-30T15:25:18.407941Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print(\"Plots saved as 'roc_curves_resnet.png' and 'confusion_matrix_resnet.png'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T15:25:19.933156Z","iopub.execute_input":"2025-03-30T15:25:19.933494Z","iopub.status.idle":"2025-03-30T15:25:19.938051Z","shell.execute_reply.started":"2025-03-30T15:25:19.933466Z","shell.execute_reply":"2025-03-30T15:25:19.937124Z"}},"outputs":[{"name":"stdout","text":"Plots saved as 'roc_curves_resnet.png' and 'confusion_matrix_resnet.png'\n","output_type":"stream"}],"execution_count":19}]}